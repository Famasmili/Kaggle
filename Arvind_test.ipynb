{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gRif0NnlZ2QP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZgd7i6sSqoB",
        "outputId": "3ebb6879-9ab8-4870-ffcd-a20799011114"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iC7NF99xZ2QT"
      },
      "outputs": [],
      "source": [
        "# Load t h e p i c k l e f i l e\n",
        "with open ( 'drive/MyDrive/Kaggle2/train_data.pkl' , 'rb' ) as f :\n",
        "    data = pickle.load ( f )\n",
        "# Access images and l a b e l s\n",
        "images = data ['images']\n",
        "labels = data ['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jQM6CvoDZ2Qb"
      },
      "outputs": [],
      "source": [
        "images=np.array(images)\n",
        "labels=np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loojftDEZ2Qe",
        "outputId": "b1efd751-1fc3-42be-c36a-95217b025279"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97477, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuIUxYYfZ2Qg",
        "outputId": "9e9ad0d9-0627-4a3c-ce12-9844dd25e178"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97477,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nysgdQuUZ2Qi",
        "outputId": "c6a890ed-7b5b-4369-f839-6804bdefb8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(97477, 784)\n"
          ]
        }
      ],
      "source": [
        "flattened_images = images.reshape(images.shape[0], -1)\n",
        "print(flattened_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HVyLtC8bZ2Qk"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "num_samples = flattened_images.shape[0]\n",
        "\n",
        "indices = np.arange(num_samples)\n",
        "np.random.shuffle(indices)\n",
        "shuffled_images = flattened_images[indices]\n",
        "shuffled_labels = labels[indices]\n",
        "\n",
        "baseline1Idxs = int(num_samples * 0.7)  # 70% for training, 30% for validation\n",
        "split_index=int(baseline1Idxs*0.7)\n",
        "\n",
        "train_images = shuffled_images[:split_index]\n",
        "train_labels = shuffled_labels[:split_index]\n",
        "\n",
        "val_images = shuffled_images[split_index:]\n",
        "val_labels = shuffled_labels[split_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHfJrEPnZ2Qp"
      },
      "outputs": [],
      "source": [
        "svm=SVMEx()\n",
        "weights_c1,history_c1=svm.fit(X_train=train_images, y_train=train_labels,\n",
        "                                X_val=val_images, y_val=val_labels,\n",
        "                        num_classes=4, epochs=100,\n",
        "                        learning_rate=0.001, C=0, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SGGHcKA6Z2Qs"
      },
      "outputs": [],
      "source": [
        "with open ( 'drive/MyDrive/Kaggle2/test_data.pkl' , 'rb' ) as f :\n",
        "    data = pickle.load ( f )\n",
        "# Access images and l a b e l s\n",
        "testimages = data ['images']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9E-_CZYZ2Qv",
        "outputId": "1984271a-8390-44b5-dc55-95590cbaabc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "testimages=np.array(testimages)\n",
        "testimages.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lIF-T2lZ2Qw"
      },
      "outputs": [],
      "source": [
        "flattened_test_images = testimages.reshape(testimages.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6kpZxGHZ2Qx"
      },
      "outputs": [],
      "source": [
        "test_preds=svm.infer(flattened_test_images,weights_c1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGq7o0LHZ2Qy",
        "outputId": "f32e7356-b100-47ae-b4de-b537741db6c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_preds==2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XvTsBifuZ2Qz"
      },
      "outputs": [],
      "source": [
        "def make_original_array(preds):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        index_column = np.arange(1, preds.size + 1)\n",
        "\n",
        "        # Reshape index_column and labels_train_modified to 2D column vectors\n",
        "        index_column = index_column.reshape(-1, 1)\n",
        "        labels_column = preds.reshape(-1, 1)\n",
        "\n",
        "        # Concatenate the index column with the labels column\n",
        "        combined_array = np.hstack((index_column, labels_column))\n",
        "\n",
        "        return combined_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utytD63EZ2Q0"
      },
      "outputs": [],
      "source": [
        "predictions=make_original_array(test_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D3VDvRkZ2Q1"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(predictions, columns=['ID', 'label'])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('SVM_baseline1_attempt.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnZuGGW7Z2Q1"
      },
      "source": [
        "## Random Forest attempt\n",
        "Parameters: 100 estimators, random_state=42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IMiUbTSZ2Q5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "rf.fit(train_images, train_labels)\n",
        "\n",
        "# Predict on validation set\n",
        "val_preds = rf.predict(val_images)\n",
        "\n",
        "# Calculate accuracy\n",
        "val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "print(f'Validation Accuracy: {val_accuracy}')\n",
        "\n",
        "# Predict on test set\n",
        "test_preds_rf = rf.predict(flattened_test_images)\n",
        "\n",
        "# Convert predictions to original array format\n",
        "predictions_rf = make_original_array(test_preds_rf)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_rf = pd.DataFrame(predictions_rf, columns=['ID', 'label'])\n",
        "df_rf.to_csv('RandomForest_baseline1_attempt.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_wH__hLZ2Q8"
      },
      "source": [
        "## Convolutional neural network attempt\n",
        "Preprocessing: Reshape images to 4D tensors, reshape labels to categorical variables\n",
        "CNN: 1st conv2d 32 channels, kernel size 3x3, relu, maxpooling 2x2, 2nd one 64 channels, kernel size 3x3, relu, maxpooling 2x2, flatten, to feed into 2 dense layers: 1st 128, relu, 2nd one 4, softmax for output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "np.random.seed(42)\n",
        "num_samples = images.shape[0]\n",
        "\n",
        "indices = np.arange(num_samples)\n",
        "np.random.shuffle(indices)\n",
        "shuffled_images = images[indices]\n",
        "shuffled_labels = labels[indices]\n",
        "\n",
        "# Split into training and validation sets\n",
        "baseline1Idxs = int(num_samples)  # 80% for training, 20% for validation\n",
        "split_index = int(baseline1Idxs * 0.8)\n",
        "\n",
        "train_images = shuffled_images[:split_index]\n",
        "train_labels = shuffled_labels[:split_index]\n",
        "\n",
        "val_images = shuffled_images[split_index:]\n",
        "val_labels = shuffled_labels[split_index:]\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"Train Images: {train_images.shape}, Train Labels: {train_labels.shape}\")\n",
        "print(f\"Validation Images: {val_images.shape}, Validation Labels: {val_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSJvDk8uTKvL",
        "outputId": "2a5407f4-44c0-4010-f7e2-268584298ef8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Images: (77981, 28, 28), Train Labels: (77981,)\n",
            "Validation Images: (19496, 28, 28), Validation Labels: (19496,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No Augmentation"
      ],
      "metadata": {
        "id": "m4A6EqqrfWuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7yZp5dxZ2Q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d40120-7e6c-4af9-dab5-29ed4acd04f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 43ms/step - accuracy: 0.6692 - loss: 0.8649 - val_accuracy: 0.7953 - val_loss: 0.5812\n",
            "Epoch 2/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 44ms/step - accuracy: 0.8009 - loss: 0.5637 - val_accuracy: 0.8130 - val_loss: 0.5175\n",
            "Epoch 3/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 40ms/step - accuracy: 0.8265 - loss: 0.4856 - val_accuracy: 0.8322 - val_loss: 0.4698\n",
            "Epoch 4/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 41ms/step - accuracy: 0.8368 - loss: 0.4573 - val_accuracy: 0.8395 - val_loss: 0.4447\n",
            "Epoch 5/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 40ms/step - accuracy: 0.8488 - loss: 0.4249 - val_accuracy: 0.8393 - val_loss: 0.4468\n",
            "Epoch 6/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 40ms/step - accuracy: 0.8555 - loss: 0.3991 - val_accuracy: 0.8458 - val_loss: 0.4323\n",
            "Epoch 7/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 39ms/step - accuracy: 0.8624 - loss: 0.3842 - val_accuracy: 0.8470 - val_loss: 0.4268\n",
            "Epoch 8/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 43ms/step - accuracy: 0.8667 - loss: 0.3707 - val_accuracy: 0.8462 - val_loss: 0.4376\n",
            "Epoch 9/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 39ms/step - accuracy: 0.8750 - loss: 0.3449 - val_accuracy: 0.8538 - val_loss: 0.4116\n",
            "Epoch 10/10\n",
            "\u001b[1m1950/1950\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 40ms/step - accuracy: 0.8833 - loss: 0.3250 - val_accuracy: 0.8538 - val_loss: 0.4123\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Reshape images to 4D tensor for CNN input\n",
        "train_images_cnn = train_images  # Assuming images are 28x28 pixels\n",
        "val_images_cnn = val_images\n",
        "#flattened_test_images_cnn = flattened_test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels_cnn = to_categorical(train_labels, num_classes=4)\n",
        "val_labels_cnn = to_categorical(val_labels, num_classes=4)\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(train_images_cnn, train_labels_cnn, epochs=10, batch_size=32, validation_data=(val_images_cnn, val_labels_cnn))\n",
        "\n",
        "# Predict on test set\n",
        "test_preds_cnn = cnn_model.predict(testimages)\n",
        "test_preds_cnn_labels = np.argmax(test_preds_cnn, axis=1)\n",
        "\n",
        "# Convert predictions to original array format\n",
        "predictions_cnn = make_original_array(test_preds_cnn_labels)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_cnn = pd.DataFrame(predictions_cnn, columns=['ID', ' Class'])\n",
        "df_cnn.to_csv('CNN_baseline2_attempt.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmenting class labels 1 and 2"
      ],
      "metadata": {
        "id": "KGtF_BPAfZZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "augment_gen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "# Filter images with labels 1 and 2\n",
        "mask = np.isin(train_labels, [1, 2])\n",
        "filtered_images = train_images[mask]\n",
        "filtered_labels = train_labels[mask]\n",
        "\n",
        "# Count the number of images with labels 1 and 2\n",
        "num_filtered_images = len(filtered_images)\n",
        "print(f\"Number of original images with labels 1 and 2: {num_filtered_images}\")\n",
        "\n",
        "\n",
        "# Augment these images\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for img, lbl in zip(filtered_images, filtered_labels):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension, shape becomes (28, 28, 1)\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension, shape becomes (1, 28, 28, 1)\n",
        "    for _ in range(5):  # Generate 5 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Convert augmented data to NumPy arrays\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "# Number of augmented images\n",
        "num_augmented_images = len(augmented_images)\n",
        "print(f\"Number of augmented images: {num_augmented_images}\")\n",
        "\n",
        "# Combine augmented data with original training data\n",
        "train_images_cnn = np.expand_dims(train_images, axis=-1)  # Add channel dimension to training images\n",
        "train_images_cnn = np.concatenate((train_images_cnn, augmented_images), axis=0)\n",
        "train_labels_cnn = np.concatenate((train_labels, augmented_labels), axis=0)\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels_cnn = to_categorical(train_labels_cnn, num_classes=4)\n",
        "val_images_cnn = np.expand_dims(val_images, axis=-1)  # Add channel dimension to validation images\n",
        "val_labels_cnn = to_categorical(val_labels, num_classes=4)\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(train_images_cnn, train_labels_cnn, epochs=10, batch_size=32, validation_data=(val_images_cnn, val_labels_cnn))\n",
        "\n",
        "\n",
        "# Predict on the validation set\n",
        "val_preds = cnn_model.predict(val_images_cnn)\n",
        "val_preds_labels = np.argmax(val_preds, axis=1)\n",
        "\n",
        "# Calculate Overall AUC\n",
        "val_labels_flat = np.argmax(val_labels_cnn, axis=1)  # Convert one-hot to class indices\n",
        "val_auc = roc_auc_score(to_categorical(val_labels_flat, num_classes=4), val_preds, multi_class=\"ovr\")\n",
        "print(f\"Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QklY0-0Yxq2g",
        "outputId": "1542c1c6-bc64-4bc6-c292-900ccd299153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - accuracy: 0.6455 - loss: 0.8443 - val_accuracy: 0.7960 - val_loss: 0.5552\n",
            "Epoch 2/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8128 - loss: 0.4838 - val_accuracy: 0.8337 - val_loss: 0.4651\n",
            "Epoch 3/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8392 - loss: 0.4172 - val_accuracy: 0.8327 - val_loss: 0.4631\n",
            "Epoch 4/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8550 - loss: 0.3805 - val_accuracy: 0.8250 - val_loss: 0.4992\n",
            "Epoch 5/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8658 - loss: 0.3504 - val_accuracy: 0.8378 - val_loss: 0.4573\n",
            "Epoch 6/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8771 - loss: 0.3220 - val_accuracy: 0.8472 - val_loss: 0.4353\n",
            "Epoch 7/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8870 - loss: 0.2994 - val_accuracy: 0.8483 - val_loss: 0.4248\n",
            "Epoch 8/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8936 - loss: 0.2787 - val_accuracy: 0.8422 - val_loss: 0.4735\n",
            "Epoch 9/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9010 - loss: 0.2605 - val_accuracy: 0.8446 - val_loss: 0.4500\n",
            "Epoch 10/10\n",
            "\u001b[1m3746/3746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9086 - loss: 0.2445 - val_accuracy: 0.8515 - val_loss: 0.4453\n",
            "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Validation AUC: 0.9396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "test_images_cnn= np.expand_dims(testimages, axis=-1)\n",
        "test_preds_cnn = cnn_model.predict(test_images_cnn)\n",
        "test_preds_cnn_labels = np.argmax(test_preds_cnn, axis=1)\n",
        "\n",
        "# Convert predictions to original array format\n",
        "predictions_cnn = make_original_array(test_preds_cnn_labels)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_cnn = pd.DataFrame(predictions_cnn, columns=['ID', ' Class'])\n",
        "df_cnn.to_csv('CNN_baseline_aug_attempt.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etM_03r8x2wA",
        "outputId": "0113bc29-9f38-4c10-bd2d-c6f1494e57ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jY5er6-LklPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "JgVkcvX-iTvQ",
        "outputId": "83768b55-a7a4-47cb-902d-88c1c82e4061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       ID   Class\n",
              "0       1       3\n",
              "1       2       0\n",
              "2       3       3\n",
              "3       4       3\n",
              "4       5       0\n",
              "..    ...     ...\n",
              "995   996       2\n",
              "996   997       1\n",
              "997   998       3\n",
              "998   999       3\n",
              "999  1000       0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80ab2e27-66a8-439c-acf3-15bce6eeca92\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>996</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>997</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>998</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>999</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80ab2e27-66a8-439c-acf3-15bce6eeca92')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80ab2e27-66a8-439c-acf3-15bce6eeca92 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80ab2e27-66a8-439c-acf3-15bce6eeca92');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e90aaa4f-3977-4d3b-93b0-4d1aaadc9db6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e90aaa4f-3977-4d3b-93b0-4d1aaadc9db6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e90aaa4f-3977-4d3b-93b0-4d1aaadc9db6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_85d861af-5f93-48d1-b83b-d9efa5c34267\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_cnn')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_85d861af-5f93-48d1-b83b-d9efa5c34267 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_cnn');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_cnn",
              "summary": "{\n  \"name\": \"df_cnn\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 288,\n        \"min\": 1,\n        \"max\": 1000,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          522,\n          738,\n          741\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_no_aug=pd.read_csv('CNN_baseline2_attempt.csv')\n",
        "cnn_no_aug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "_6fPK2qSijQH",
        "outputId": "a82185a3-7968-437b-ac6d-5551d9b20863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       ID   Class\n",
              "0       1       3\n",
              "1       2       0\n",
              "2       3       3\n",
              "3       4       3\n",
              "4       5       0\n",
              "..    ...     ...\n",
              "995   996       0\n",
              "996   997       1\n",
              "997   998       3\n",
              "998   999       3\n",
              "999  1000       0\n",
              "\n",
              "[1000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ba5425b-a891-49ce-936d-fbd228aaa865\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>996</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>997</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>998</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>999</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ba5425b-a891-49ce-936d-fbd228aaa865')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6ba5425b-a891-49ce-936d-fbd228aaa865 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6ba5425b-a891-49ce-936d-fbd228aaa865');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ac401177-441a-436f-bc61-e4ee107aeef1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ac401177-441a-436f-bc61-e4ee107aeef1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ac401177-441a-436f-bc61-e4ee107aeef1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a58911bf-a7dc-41b7-b7e1-cf860787a60e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('cnn_no_aug')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a58911bf-a7dc-41b7-b7e1-cf860787a60e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('cnn_no_aug');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "cnn_no_aug",
              "summary": "{\n  \"name\": \"cnn_no_aug\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 288,\n        \"min\": 1,\n        \"max\": 1000,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          522,\n          738,\n          741\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \" Class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the predicted classes are in a column named 'Class'\n",
        "differences = (df_cnn[' Class'] != cnn_no_aug[' Class']).sum()\n",
        "total = len(df_cnn)\n",
        "\n",
        "print(f\"Total Predictions: {total}\")\n",
        "print(f\"Different Predictions: {differences}\")\n",
        "print(f\"Percentage Difference: {(differences / total) * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsvwxkgHixp-",
        "outputId": "58419228-9dd2-440b-f00f-5313e39c49ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Predictions: 1000\n",
            "Different Predictions: 203\n",
            "Percentage Difference: 20.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More augmentation for class 1 and 2 and a little for class 0 and 3"
      ],
      "metadata": {
        "id": "qVtHWt0HffOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "augment_gen = ImageDataGenerator(\n",
        "    rotation_range=30,          # Increase rotation\n",
        "    width_shift_range=0.2,      # Increase horizontal shift\n",
        "    height_shift_range=0.2,     # Increase vertical shift\n",
        "    zoom_range=0.2,             # Add zoom augmentation\n",
        "    shear_range=15,             # Add shearing\n",
        "    horizontal_flip=True,       # Allow horizontal flips\n",
        "    fill_mode='nearest'         # Fill missing pixels\n",
        ")\n",
        "\n",
        "# Filter images with labels 1 and 2\n",
        "mask1 = np.isin(train_labels, [1, 2])\n",
        "filtered_images = train_images[mask1]\n",
        "filtered_labels = train_labels[mask1]\n",
        "\n",
        "# Count the number of images with labels 1 and 2\n",
        "num_filtered_images = len(filtered_images)\n",
        "print(f\"Number of original images with labels 1 and 2: {num_filtered_images}\")\n",
        "\n",
        "\n",
        "# Augment these images\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for img, lbl in zip(filtered_images, filtered_labels):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension, shape becomes (28, 28, 1)\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension, shape becomes (1, 28, 28, 1)\n",
        "    for _ in range(10):  # Generate 5 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "mask2 = np.isin(train_labels, [0, 3])\n",
        "filtered_images = train_images[mask2]\n",
        "filtered_labels = train_labels[mask2]\n",
        "\n",
        "# Count the number of images with labels 1 and 2\n",
        "num_filtered_images = len(filtered_images)\n",
        "print(f\"Number of original images with labels 0 and 3: {num_filtered_images}\")\n",
        "\n",
        "\n",
        "for img, lbl in zip(filtered_images, filtered_labels):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension, shape becomes (28, 28, 1)\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension, shape becomes (1, 28, 28, 1)\n",
        "    for _ in range(2):  # Generate 5 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "\n",
        "# Convert augmented data to NumPy arrays\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "# Number of augmented images\n",
        "num_augmented_images = len(augmented_images)\n",
        "print(f\"Number of augmented images: {num_augmented_images}\")\n",
        "\n",
        "# Combine augmented data with original training data\n",
        "train_images_cnn = np.expand_dims(train_images, axis=-1)  # Add channel dimension to training images\n",
        "train_images_cnn = np.concatenate((train_images_cnn, augmented_images), axis=0)\n",
        "train_labels_cnn = np.concatenate((train_labels, augmented_labels), axis=0)\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels_cnn = to_categorical(train_labels_cnn, num_classes=4)\n",
        "val_images_cnn = np.expand_dims(val_images, axis=-1)  # Add channel dimension to validation images\n",
        "val_labels_cnn = to_categorical(val_labels, num_classes=4)\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(train_images_cnn, train_labels_cnn, epochs=15, batch_size=32, validation_data=(val_images_cnn, val_labels_cnn))\n",
        "\n",
        "\n",
        "# Predict on the validation set\n",
        "val_preds = cnn_model.predict(val_images_cnn)\n",
        "val_preds_labels = np.argmax(val_preds, axis=1)\n",
        "\n",
        "# Calculate Overall AUC\n",
        "val_labels_flat = np.argmax(val_labels_cnn, axis=1)  # Convert one-hot to class indices\n",
        "val_auc = roc_auc_score(to_categorical(val_labels_flat, num_classes=4), val_preds, multi_class=\"ovr\")\n",
        "print(f\"Validation AUC: {val_auc:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHcKk8g4lCXJ",
        "outputId": "ed75ee9c-6f93-4dcb-8203-22f67903c314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of original images with labels 1 and 2: 11497\n",
            "Number of original images with labels 0 and 3: 50887\n",
            "Number of augmented images: 216744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.5611 - loss: 1.0147 - val_accuracy: 0.8044 - val_loss: 0.5264\n",
            "Epoch 2/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.6925 - loss: 0.7568 - val_accuracy: 0.8300 - val_loss: 0.4737\n",
            "Epoch 3/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.7211 - loss: 0.6916 - val_accuracy: 0.8354 - val_loss: 0.4570\n",
            "Epoch 4/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.7395 - loss: 0.6488 - val_accuracy: 0.8428 - val_loss: 0.4418\n",
            "Epoch 5/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - accuracy: 0.7518 - loss: 0.6189 - val_accuracy: 0.8491 - val_loss: 0.4207\n",
            "Epoch 6/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - accuracy: 0.7640 - loss: 0.5931 - val_accuracy: 0.8481 - val_loss: 0.4210\n",
            "Epoch 7/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.7706 - loss: 0.5727 - val_accuracy: 0.8522 - val_loss: 0.4256\n",
            "Epoch 8/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.7799 - loss: 0.5531 - val_accuracy: 0.8491 - val_loss: 0.4184\n",
            "Epoch 9/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - accuracy: 0.7876 - loss: 0.5349 - val_accuracy: 0.8465 - val_loss: 0.4252\n",
            "Epoch 10/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.7947 - loss: 0.5177 - val_accuracy: 0.8536 - val_loss: 0.4154\n",
            "Epoch 11/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.8001 - loss: 0.5020 - val_accuracy: 0.8534 - val_loss: 0.4190\n",
            "Epoch 12/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.8053 - loss: 0.4882 - val_accuracy: 0.8410 - val_loss: 0.4479\n",
            "Epoch 13/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.8128 - loss: 0.4734 - val_accuracy: 0.8507 - val_loss: 0.4289\n",
            "Epoch 14/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.8172 - loss: 0.4601 - val_accuracy: 0.8515 - val_loss: 0.4345\n",
            "Epoch 15/15\n",
            "\u001b[1m8723/8723\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.8220 - loss: 0.4492 - val_accuracy: 0.8492 - val_loss: 0.4614\n",
            "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Validation AUC: 0.9390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test set\n",
        "test_images_cnn= np.expand_dims(testimages, axis=-1)\n",
        "test_preds_cnn = cnn_model.predict(test_images_cnn)\n",
        "test_preds_cnn_labels = np.argmax(test_preds_cnn, axis=1)\n",
        "\n",
        "# Convert predictions to original array format\n",
        "predictions_cnn = make_original_array(test_preds_cnn_labels)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_cnn_augb = pd.DataFrame(predictions_cnn, columns=['ID', ' Class'])\n",
        "df_cnn_augb.to_csv('CNN_baseline_augboth_attempt.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBL6kygBnXiI",
        "outputId": "803d4c7c-0d45-41e8-a3e3-a23bafa125dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the predicted classes are in a column named 'Class'\n",
        "differences = (df_cnn[' Class'] != df_cnn_augb[' Class']).sum()\n",
        "total = len(df_cnn)\n",
        "\n",
        "print(f\"Total Predictions: {total}\")\n",
        "print(f\"Different Predictions: {differences}\")\n",
        "print(f\"Percentage Difference: {(differences / total) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XlLCrwUnfDO",
        "outputId": "b98fb9de-5f34-486b-9219-22fa7aec22dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Predictions: 1000\n",
            "Different Predictions: 221\n",
            "Percentage Difference: 22.10%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FxBgdIynvAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lQLcD7e4nmcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmZSpjRVZ2Q-"
      },
      "source": [
        "**Same model using Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vFKjhhcZ2Q_",
        "outputId": "b32454f7-e757-481b-eeb0-7d3cb8cc4764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6258089580936321, Accuracy: 0.7795993080420003\n",
            "Epoch 2/10, Loss: 0.5465488908872335, Accuracy: 0.8069759021603572\n",
            "Epoch 3/10, Loss: 0.5201600855036592, Accuracy: 0.8150018103552319\n",
            "Epoch 4/10, Loss: 0.4983069887636481, Accuracy: 0.822927143259444\n",
            "Epoch 5/10, Loss: 0.4930240340252794, Accuracy: 0.8230277185501066\n",
            "Epoch 6/10, Loss: 0.47727027386810483, Accuracy: 0.8315162730820292\n",
            "Epoch 7/10, Loss: 0.4605699416676697, Accuracy: 0.836826648429014\n",
            "Epoch 8/10, Loss: 0.45732239249283435, Accuracy: 0.836283541859436\n",
            "Epoch 9/10, Loss: 0.4551902298924622, Accuracy: 0.8389990747073259\n",
            "Epoch 10/10, Loss: 0.44121442443692394, Accuracy: 0.8434646176127449\n",
            "AUC for class 0: 0.9798477641800922\n",
            "AUC for class 1: 0.9469241140223681\n",
            "AUC for class 2: 0.8418766190799161\n",
            "AUC for class 3: 0.9601584625074469\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Reshape images to 4D tensor for CNN input\n",
        "train_images_cnn = train_images.reshape(-1, 1, 28, 28)  # Assuming images are 28x28 pixels\n",
        "val_images_cnn = val_images.reshape(-1, 1, 28, 28)\n",
        "flattened_test_images_cnn = flattened_test_images.reshape(-1, 1, 28, 28)\n",
        "\n",
        "# Convert labels to categorical\n",
        "train_labels_cnn = torch.tensor(train_labels, dtype=torch.long)\n",
        "val_labels_cnn = torch.tensor(val_labels, dtype=torch.long)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_images_tensor = torch.tensor(train_images_cnn, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "val_images_tensor = torch.tensor(val_images_cnn, dtype=torch.float32).to(device)\n",
        "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long).to(device)\n",
        "test_images_tensor = torch.tensor(flattened_test_images_cnn, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "val_dataset = TensorDataset(val_images_tensor, val_labels_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the CNN model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "cnn_model = CNNModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    cnn_model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validate the model\n",
        "    cnn_model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = cnn_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = val_correct / len(val_dataset)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss}, Accuracy: {val_accuracy}')\n",
        "\n",
        "# Save the model state\n",
        "torch.save(cnn_model.state_dict(), 'cnn_model.pth')\n",
        "\n",
        "# Compute AUC for each class\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "auc_scores = []\n",
        "for i in range(4):\n",
        "    auc = roc_auc_score(all_labels == i, all_preds[:, i])\n",
        "    auc_scores.append(auc)\n",
        "    print(f'AUC for class {i}: {auc}')\n",
        "\n",
        "# Predict on test set\n",
        "cnn_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = cnn_model(test_images_tensor)\n",
        "    _, test_preds_cnn_labels = torch.max(test_outputs, 1)\n",
        "\n",
        "# Convert predictions to original array format\n",
        "predictions_cnn = make_original_array(test_preds_cnn_labels.cpu().numpy())\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_cnn = pd.DataFrame(predictions_cnn, columns=['ID', 'label'])\n",
        "df_cnn.to_csv('CNN_baseline1_attempt.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UluxsIaxZ2RB"
      },
      "source": [
        "# This cell defines and compiles a ResNet50 model using TensorFlow and Keras.\n",
        "# The model is designed for image classification tasks and consists of the following components:\n",
        "#\n",
        "# 1. **Base Model**:\n",
        "#    - Architecture: ResNet50\n",
        "#    - Pre-trained Weights: ImageNet\n",
        "#    - Include Top: False (excluding the top fully connected layer)\n",
        "#    - Input Shape: (28, 28, 3) (assuming input images are 28x28 pixels with 3 color channels)\n",
        "#\n",
        "# 2. **Custom Layers**:\n",
        "#    - GlobalAveragePooling2D: Reduces each feature map to a single value.\n",
        "#    - Dense Layer:\n",
        "#      - Units: 128\n",
        "#      - Activation: ReLU\n",
        "#    - Dropout Layer:\n",
        "#      - Rate: 0.5 (to prevent overfitting)\n",
        "#    - Dense Layer:\n",
        "#      - Units: 4 (assuming 4 classes for classification)\n",
        "#      - Activation: Softmax\n",
        "#\n",
        "# The model is compiled with the following parameters:\n",
        "# - **Optimizer**: Adam\n",
        "# - **Loss Function**: Categorical Crossentropy (suitable for multi-class classification)\n",
        "# - **Metrics**: Accuracy (to monitor the accuracy during training and evaluation)\n",
        "#\n",
        "# Design Choices:\n",
        "# - The use of ResNet50 as the base model leverages pre-trained weights to improve feature extraction.\n",
        "# - GlobalAveragePooling2D is used to reduce the spatial dimensions of the feature maps.\n",
        "# - ReLU activation is chosen for its effectiveness in introducing non-linearity.\n",
        "# - Dropout is applied to prevent overfitting.\n",
        "# - The final Dense layer uses Softmax activation to output a probability distribution over the classes.\n",
        "# - Adam optimizer is selected for its efficiency and adaptive learning rate capabilities.\n",
        "# - Categorical Crossentropy is used as the loss function to handle one-hot encoded labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WblVWodOZ2RC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the ResNet50 model with pre-trained weights, excluding the top layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(28, 28, 3))\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Convert grayscale images to 3 channels by repeating the single channel 3 times\n",
        "train_images_resnet = np.repeat(train_images_cnn, 3, axis=-1)\n",
        "val_images_resnet = np.repeat(val_images_cnn, 3, axis=-1)\n",
        "flattened_test_images_resnet = np.repeat(flattened_test_images_cnn, 3, axis=-1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images_resnet, train_labels_cnn, epochs=10, batch_size=32, validation_data=(val_images_resnet, val_labels_cnn))\n",
        "\n",
        "# Predict on test set\n",
        "test_preds_resnet = model.predict(flattened_test_images_resnet)\n",
        "test_preds_resnet_labels = np.argmax(test_preds_resnet, axis=1)\n",
        "\n",
        "# Convert predictions to original array format\n",
        "predictions_resnet = make_original_array(test_preds_resnet_labels)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_resnet = pd.DataFrame(predictions_resnet, columns=['ID', 'label'])\n",
        "df_resnet.to_csv('ResNet_baseline1_attempt.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOwvYBfhZ2RD"
      },
      "source": [
        "## ResNet50 attempt using PyTorch\n",
        "Parameters: 100 epochs, learning rate=0.001, batch size=32, dropout rate=0.5\n",
        "\n",
        "Preprocessing: Convert grayscale images to 3 channels by repeating the single channel 3 times\n",
        "\n",
        "Model Architecture:\n",
        "- Base Model: ResNet50 with pre-trained weights (ImageNet)\n",
        "- Custom Layers:\n",
        "    - Global Average Pooling\n",
        "    - Dense Layer: 128 units, ReLU activation\n",
        "    - Dropout Layer: 0.5 rate\n",
        "    - Dense Layer: 4 units, Softmax activation\n",
        "\n",
        "Training:\n",
        "- Loss Function: CrossEntropyLoss\n",
        "- Optimizer: Adam\n",
        "\n",
        "Evaluation:\n",
        "- Validation Accuracy and Loss per epoch\n",
        "- Predictions on test set saved to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOqS6rLcZ2RE"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Load the ResNet50 model with pre-trained weights for retina medical images\n",
        "base_model = models.resnet50(weights='IMAGENET1K_V2').to(device)\n",
        "base_model.fc = nn.Identity()  # Remove the final fully connected layer\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(CustomResNet, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc1 = nn.Linear(base_model.fc.in_features, 128)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CustomResNet(base_model).to(device)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for param in base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Convert grayscale images to 3 channels by repeating the single channel 3 times\n",
        "train_images_resnet = np.repeat(train_images_cnn, 3, axis=1)\n",
        "val_images_resnet = np.repeat(val_images_cnn, 3, axis=1)\n",
        "flattened_test_images_resnet = np.repeat(flattened_test_images_cnn, 3, axis=1)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_images_tensor = torch.tensor(train_images_resnet, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "val_images_tensor = torch.tensor(val_images_resnet, dtype=torch.float32).to(device)\n",
        "val_labels_tensor = torch.tensor(val_labels, dtype=torch.long).to(device)\n",
        "test_images_tensor = torch.tensor(flattened_test_images_resnet, dtype=torch.float32).to(device)\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
        "val_dataset = TensorDataset(val_images_tensor, val_labels_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "val_accuracies = []\n",
        "val_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = val_correct / len(val_dataset)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss}, Accuracy: {val_accuracy}')\n",
        "\n",
        "# Save the model state\n",
        "torch.save(model.state_dict(), 'resnet_model.pth')\n",
        "\n",
        "# Compute AUC for each class\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "auc_scores = []\n",
        "for i in range(4):\n",
        "    auc = roc_auc_score(all_labels == i, all_preds[:, i])\n",
        "    auc_scores.append(auc)\n",
        "    print(f'AUC for class {i}: {auc}')\n",
        "\n",
        "# Predict on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model(test_images_tensor)\n",
        "    _, test_preds_resnet_labels = torch.max(test_outputs, 1)\n",
        "\n",
        "# Convert predictions to original array format\n",
        "predictions_resnet = make_original_array(test_preds_resnet_labels.cpu().numpy())\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_resnet = pd.DataFrame(predictions_resnet, columns=['ID', 'label'])\n",
        "df_resnet.to_csv('ResNet_baseline1_attempt.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9D14alGZ2RF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trying out vision transfomers and performing augmentations"
      ],
      "metadata": {
        "id": "sz3rQy87NZIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVJPQX99QWQ4",
        "outputId": "791a2542-7cec-402c-d47e-1830d8889c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "import numpy as np\n",
        "\n",
        "# Custom Dataset with Augmentation\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None, augment_classes=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.augment_classes = augment_classes\n",
        "        self.augmented_images = []\n",
        "        self.augmented_labels = []\n",
        "\n",
        "        # Perform augmentations for specified classes\n",
        "        if self.augment_classes:\n",
        "            for i, label in enumerate(labels):\n",
        "                if label in self.augment_classes:\n",
        "                    augmented_image = self.apply_augmentation(images[i])\n",
        "                    self.augmented_images.append(augmented_image)\n",
        "                    self.augmented_labels.append(label)\n",
        "            # Convert augmented images to the same shape as original images\n",
        "            self.augmented_images = np.array(self.augmented_images).squeeze()  # Remove extra channel if added\n",
        "            self.images = np.concatenate((self.images, self.augmented_images), axis=0)\n",
        "            self.labels = np.concatenate((self.labels, np.array(self.augmented_labels)), axis=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "    def apply_augmentation(self, image):\n",
        "        \"\"\"Apply augmentations and return an image with the same dimensions.\"\"\"\n",
        "        aug_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        augmented_image = aug_transform(image)\n",
        "        return augmented_image.squeeze(0).numpy()  # Convert back to (height, width)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize for 3 channels\n",
        "])\n",
        "\n",
        "# Dataset Preparation\n",
        "dataset = CustomDataset(images=images, labels=labels, transform=transform, augment_classes=[1, 2])\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Vision Transformer Model\n",
        "class ViTModel28x28(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(ViTModel28x28, self).__init__()\n",
        "        # Use vit_base_patch16_224 and adjust img_size\n",
        "        self.vit = timm.create_model(\n",
        "            'vit_base_patch16_224',\n",
        "            pretrained=True,\n",
        "            img_size=28,  # Input size\n",
        "            patch_size=7  # Adjust patch size for 28x28 images\n",
        "        )\n",
        "        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)  # Replace the head\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "# Initialize Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ViTModel28x28(num_classes=4).to(device)\n",
        "\n",
        "\n",
        "# Initialize Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ViTModel28x28(num_classes=4).to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to Calculate AUC\n",
        "def calculate_auc(loader, model, num_classes=4):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probabilities = nn.Softmax(dim=1)(outputs)\n",
        "\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            all_outputs.append(probabilities.cpu().numpy())\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_outputs = np.concatenate(all_outputs)\n",
        "\n",
        "    # One-hot encode labels\n",
        "    one_hot_labels = np.eye(num_classes)[all_labels]\n",
        "\n",
        "    # Calculate AUC\n",
        "    auc = roc_auc_score(one_hot_labels, all_outputs, multi_class=\"ovr\")\n",
        "    return auc\n",
        "\n",
        "# Training Loop\n",
        "def train_vit(model, train_loader, val_loader, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Backward Pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accuracy\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_accuracy = train_correct / len(train_loader.dataset)\n",
        "        val_accuracy = val_correct / len(val_loader.dataset)\n",
        "        val_auc = calculate_auc(val_loader, model)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Val Accuracy: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "# Train the Model\n",
        "train_vit(model, train_loader, val_loader, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "t-d2CviVOdKd",
        "outputId": "2a70fe59-ba5b-40d1-b692-f7cbc1cd3a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 3637.1604, Train Accuracy: 0.4629, Val Accuracy: 0.5231, Val AUC: 0.6864\n",
            "Epoch 2/10, Train Loss: 2980.7136, Train Accuracy: 0.5943, Val Accuracy: 0.6387, Val AUC: 0.8030\n",
            "Epoch 3/10, Train Loss: 2597.0436, Train Accuracy: 0.6579, Val Accuracy: 0.6701, Val AUC: 0.8355\n",
            "Epoch 4/10, Train Loss: 2443.5494, Train Accuracy: 0.6780, Val Accuracy: 0.6436, Val AUC: 0.8135\n",
            "Epoch 5/10, Train Loss: 2603.2740, Train Accuracy: 0.6545, Val Accuracy: 0.6837, Val AUC: 0.8417\n",
            "Epoch 6/10, Train Loss: 2613.0445, Train Accuracy: 0.6512, Val Accuracy: 0.6843, Val AUC: 0.8527\n",
            "Epoch 7/10, Train Loss: 2448.0301, Train Accuracy: 0.6743, Val Accuracy: 0.6924, Val AUC: 0.8577\n",
            "Epoch 8/10, Train Loss: 2422.3131, Train Accuracy: 0.6776, Val Accuracy: 0.6717, Val AUC: 0.8388\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7a9fffc7c72b>\u001b[0m in \u001b[0;36m<cell line: 170>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;31m# Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0mtrain_vit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-7a9fffc7c72b>\u001b[0m in \u001b[0;36mtrain_vit\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Backward Pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet-50 from scratch"
      ],
      "metadata": {
        "id": "4Isf7RFLsOwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "augment_gen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Augment images with labels 1 and 2\n",
        "mask1 = np.isin(train_labels, [1, 2])\n",
        "filtered_images_1_2 = train_images[mask1]\n",
        "filtered_labels_1_2 = train_labels[mask1]\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for img, lbl in zip(filtered_images_1_2, filtered_labels_1_2):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension, shape becomes (28, 28, 1)\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension, shape becomes (1, 28, 28, 1)\n",
        "    for _ in range(10):  # Generate 10 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Augment images with labels 0 and 3\n",
        "mask2 = np.isin(train_labels, [0, 3])\n",
        "filtered_images_0_3 = train_images[mask2]\n",
        "filtered_labels_0_3 = train_labels[mask2]\n",
        "\n",
        "for img, lbl in zip(filtered_images_0_3, filtered_labels_0_3):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension, shape becomes (28, 28, 1)\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension, shape becomes (1, 28, 28, 1)\n",
        "    for _ in range(2):  # Generate 2 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Convert augmented data to NumPy arrays\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "\n",
        "# Combine augmented data with original training data\n",
        "train_images_cnn = np.expand_dims(train_images, axis=-1)  # Add channel dimension to training images\n",
        "train_images_cnn = np.concatenate((train_images_cnn, augmented_images), axis=0)\n",
        "train_labels_cnn = np.concatenate((train_labels, augmented_labels), axis=0)\n",
        "\n",
        "# Convert labels to categorical\n",
        "\n",
        "val_images_cnn = np.expand_dims(val_images, axis=-1)  # Add channel dimension to validation images\n",
        "train_labels_cnn = np.argmax(to_categorical(train_labels_cnn, num_classes=4), axis=1)\n",
        "val_labels_cnn = np.argmax(to_categorical(val_labels, num_classes=4), axis=1)\n",
        "\n",
        "\n",
        "# PyTorch Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_images_cnn, train_labels_cnn, transform=transform)\n",
        "val_dataset = CustomDataset(val_images, val_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Bottleneck Block for ResNet-50\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet-50 for 28x28 Inputs\n",
        "class ResNet28x28(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=4):\n",
        "        super(ResNet28x28, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Adjusted for 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1)  # No stride for 28x28 input\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * 4\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Training Loop with AUC Calculation\n",
        "def train_resnet28(model, train_loader, val_loader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "\n",
        "                # Store predicted probabilities and true class indices\n",
        "                all_preds.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate AUC\n",
        "        val_auc = roc_auc_score(\n",
        "            np.eye(4)[np.array(all_labels)],  # Convert class indices to one-hot encoded for AUC\n",
        "            np.array(all_preds),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, \"\n",
        "              f\"Train Accuracy: {train_correct / len(train_loader.dataset):.4f}, \"\n",
        "              f\"Val Accuracy: {val_correct / len(val_loader.dataset):.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "# Instantiate Model and Train\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet28x28(Bottleneck, [3, 4, 6, 3], num_classes=4).to(device)\n",
        "\n",
        "train_resnet28(model, train_loader, val_loader, epochs=15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "4DlGH9mVsyMV",
        "outputId": "56bfc2e7-cab3-4a0b-fcec-eb7b5561105f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Train Loss: 6419.6298, Train Accuracy: 0.7022, Val Loss: 467.4983, Val Accuracy: 0.8462, Val AUC: 0.9455\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f55dc3a6ba98>\u001b[0m in \u001b[0;36m<cell line: 234>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet28x28\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m \u001b[0mtrain_resnet28\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-f55dc3a6ba98>\u001b[0m in \u001b[0;36mtrain_resnet28\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mtrain_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Validation Phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BEST ONE SO FAR"
      ],
      "metadata": {
        "id": "h_6Kjv2eFe4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Data Augmentation\n",
        "augment_gen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Augment images with labels 1 and 2\n",
        "mask1 = np.isin(train_labels, [1, 2])\n",
        "filtered_images_1_2 = train_images[mask1]\n",
        "filtered_labels_1_2 = train_labels[mask1]\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for img, lbl in zip(filtered_images_1_2, filtered_labels_1_2):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
        "    for _ in range(10):  # Generate 10 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Augment images with labels 0 and 3\n",
        "mask2 = np.isin(train_labels, [0, 3])\n",
        "filtered_images_0_3 = train_images[mask2]\n",
        "filtered_labels_0_3 = train_labels[mask2]\n",
        "\n",
        "for img, lbl in zip(filtered_images_0_3, filtered_labels_0_3):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
        "    for _ in range(2):  # Generate 2 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Combine augmented data with original training data\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "train_images_cnn = np.expand_dims(train_images, axis=-1)  # Add channel dimension to training images\n",
        "train_images_cnn = np.concatenate((train_images_cnn, augmented_images), axis=0)\n",
        "train_labels_cnn = np.concatenate((train_labels, augmented_labels), axis=0)\n",
        "\n",
        "# Convert labels to class indices\n",
        "train_labels_cnn = train_labels_cnn.astype(int)\n",
        "val_labels = val_labels.astype(int)\n",
        "\n",
        "# PyTorch Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_images_cnn, train_labels_cnn, transform=transform)\n",
        "val_dataset = CustomDataset(val_images, val_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Bottleneck Block for ResNet-50\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet-50 for 28x28 Inputs\n",
        "class ResNet28x28(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=4):\n",
        "        super(ResNet28x28, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Adjusted for 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1)  # No stride for 28x28 input\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * 4\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Compute Class Weights for Balanced Loss\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_cnn), y=train_labels_cnn)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "# Temperature Scaled Cross Entropy Loss\n",
        "class TemperatureScaledCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, temperature=2.0):\n",
        "        super(TemperatureScaledCrossEntropyLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        scaled_logits = logits / self.temperature\n",
        "        return self.criterion(scaled_logits, targets)\n",
        "\n",
        "# Training Loop with AUC Calculation and Dynamic LR Adjustment\n",
        "# Training Loop with AUC Calculation and Dynamic LR Adjustment\n",
        "def train_resnet28(model, train_loader, val_loader, epochs=15):\n",
        "    # Optimizer and Criterion\n",
        "    criterion = TemperatureScaledCrossEntropyLoss(temperature=2.0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    # Learning Rate Scheduler Placeholder\n",
        "    scheduler_active = False  # To ensure we reduce LR only after crossing accuracy\n",
        "    reduced_lr = 1e-5  # New learning rate after crossing 85% accuracy\n",
        "    reduced_lr_2=3e-6\n",
        "    reduced_lr_3=1e-6\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Ensure tensors are on the correct device\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)  # Explicitly move to device\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "\n",
        "                # Store predicted probabilities and true class indices\n",
        "                all_preds.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate AUC\n",
        "        val_auc = roc_auc_score(\n",
        "            np.eye(4)[np.array(all_labels)],  # Convert class indices to one-hot encoded for AUC\n",
        "            np.array(all_preds),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "\n",
        "        # Metrics\n",
        "        train_accuracy = train_correct / len(train_loader.dataset)\n",
        "        val_accuracy = val_correct / len(val_loader.dataset)\n",
        "        new_lr = None\n",
        "        if train_accuracy > 0.91:\n",
        "            new_lr = reduced_lr_3\n",
        "            print(f\"Learning rate reduced to {new_lr:.1e} after epoch {epoch+1}\")\n",
        "        elif train_accuracy > 0.88:\n",
        "            new_lr = reduced_lr_2\n",
        "            print(f\"Learning rate reduced to {new_lr:.1e} after epoch {epoch+1}\")\n",
        "        elif train_accuracy > 0.85 and not scheduler_active:\n",
        "            new_lr = reduced_lr\n",
        "            scheduler_active = True  # Only adjust once for this threshold\n",
        "            print(f\"Learning rate reduced to {new_lr:.1e} after epoch {epoch+1}\")\n",
        "        else:\n",
        "            new_lr = None\n",
        "\n",
        "        # Update learning rate if changed\n",
        "        if new_lr is not None:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = new_lr\n",
        "\n",
        "        # Print epoch metrics\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "# Instantiate Model and Train\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "assert torch.cuda.is_available(), \"CUDA is not available. Ensure GPU drivers are installed and CUDA is supported.\"\n",
        "model = ResNet28x28(Bottleneck, [3, 4, 6, 3], num_classes=4).to(device)\n",
        "assert next(model.parameters()).is_cuda, \"Model is not on GPU!\"\n",
        "train_resnet28(model, train_loader, val_loader, epochs=15)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5FVZF0_s9E7",
        "outputId": "8b6ba12b-d428-43cb-a4da-a005a21e1198"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/15, Train Loss: 15093.3895, Train Accuracy: 0.7248, Val Loss: 508.8144, Val Accuracy: 0.8400, Val AUC: 0.9513\n",
            "Epoch 2/15, Train Loss: 11715.1383, Train Accuracy: 0.7913, Val Loss: 447.9826, Val Accuracy: 0.8827, Val AUC: 0.9612\n",
            "Epoch 3/15, Train Loss: 10592.5576, Train Accuracy: 0.8137, Val Loss: 415.3051, Val Accuracy: 0.8857, Val AUC: 0.9653\n",
            "Epoch 4/15, Train Loss: 9849.7282, Train Accuracy: 0.8278, Val Loss: 427.4391, Val Accuracy: 0.8730, Val AUC: 0.9636\n",
            "Epoch 5/15, Train Loss: 9236.1505, Train Accuracy: 0.8384, Val Loss: 384.7570, Val Accuracy: 0.8937, Val AUC: 0.9687\n",
            "Epoch 6/15, Train Loss: 8742.9923, Train Accuracy: 0.8481, Val Loss: 377.3969, Val Accuracy: 0.8968, Val AUC: 0.9700\n",
            "Learning rate reduced to 1.0e-05 after epoch 7\n",
            "Epoch 7/15, Train Loss: 8247.8865, Train Accuracy: 0.8576, Val Loss: 377.4650, Val Accuracy: 0.9003, Val AUC: 0.9715\n",
            "Learning rate reduced to 3.0e-06 after epoch 8\n",
            "Epoch 8/15, Train Loss: 6123.6544, Train Accuracy: 0.8964, Val Loss: 362.8836, Val Accuracy: 0.9098, Val AUC: 0.9743\n",
            "Learning rate reduced to 3.0e-06 after epoch 9\n",
            "Epoch 9/15, Train Loss: 5425.1496, Train Accuracy: 0.9092, Val Loss: 373.5442, Val Accuracy: 0.9118, Val AUC: 0.9736\n",
            "Learning rate reduced to 1.0e-06 after epoch 10\n",
            "Epoch 10/15, Train Loss: 5246.7370, Train Accuracy: 0.9128, Val Loss: 375.3222, Val Accuracy: 0.9122, Val AUC: 0.9738\n",
            "Learning rate reduced to 1.0e-06 after epoch 11\n",
            "Epoch 11/15, Train Loss: 5049.6659, Train Accuracy: 0.9166, Val Loss: 382.6457, Val Accuracy: 0.9111, Val AUC: 0.9730\n",
            "Learning rate reduced to 1.0e-06 after epoch 12\n",
            "Epoch 12/15, Train Loss: 5002.4649, Train Accuracy: 0.9175, Val Loss: 387.8848, Val Accuracy: 0.9092, Val AUC: 0.9734\n",
            "Learning rate reduced to 1.0e-06 after epoch 13\n",
            "Epoch 13/15, Train Loss: 4942.4981, Train Accuracy: 0.9183, Val Loss: 387.5853, Val Accuracy: 0.9100, Val AUC: 0.9731\n",
            "Learning rate reduced to 1.0e-06 after epoch 14\n",
            "Epoch 14/15, Train Loss: 4900.6509, Train Accuracy: 0.9197, Val Loss: 397.5801, Val Accuracy: 0.9100, Val AUC: 0.9731\n",
            "Learning rate reduced to 1.0e-06 after epoch 15\n",
            "Epoch 15/15, Train Loss: 4850.7179, Train Accuracy: 0.9200, Val Loss: 389.5430, Val Accuracy: 0.9103, Val AUC: 0.9732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of augmented images: {augmented_images.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09rjpWNh3FuN",
        "outputId": "40f3d4ba-6b78-4269-ca23-ee83c965de81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of augmented images: 270802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images_cnn= np.expand_dims(testimages, axis=-1)\n",
        "test_preds_cnn = cnn_model.predict(test_images_cnn)\n",
        "test_preds_cnn_labels = np.argmax(test_preds_cnn, axis=1)\n",
        "\n",
        "# Convert predictions to original array format\n",
        "predictions_cnn = make_original_array(test_preds_cnn_labels)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_cnn_augb = pd.DataFrame(predictions_cnn, columns=['ID', ' Class'])\n",
        "df_cnn_augb.to_csv('CNN_baseline_augboth_attempt.csv', index=False)"
      ],
      "metadata": {
        "id": "rJpq-rvkO_7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "# Assuming testimages is a NumPy array of shape (num_samples, 28, 28)\n",
        "\n",
        "# Preprocess test images\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Ensure same normalization as during training\n",
        "])\n",
        "\n",
        "# Convert testimages to PyTorch Tensor with proper dimensions\n",
        "test_images_tensor = torch.stack([test_transform(img) for img in testimages])  # Shape: (num_samples, 1, 28, 28)\n",
        "\n",
        "# Send to device (e.g., GPU or CPU)\n",
        "test_images_tensor = test_images_tensor.to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    logits = model(test_images_tensor)  # Shape: (num_samples, num_classes)\n",
        "    probabilities = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
        "    predicted_classes = torch.argmax(probabilities, dim=1)  # Get class with highest probability\n",
        "\n",
        "# Convert predictions to NumPy array\n",
        "predicted_classes_np = predicted_classes.cpu().numpy()\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predicted Classes:\", predicted_classes_np)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K99pEmIGP51T",
        "outputId": "a58fc1ba-c46b-4bb1-9273-ec470c6247a1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Classes: [3 0 3 3 0 2 1 0 0 3 0 1 3 3 3 2 0 3 0 0 1 0 0 3 0 0 0 1 3 1 2 1 0 3 0 1 1\n",
            " 3 1 0 3 2 0 0 3 0 3 0 0 3 1 0 1 0 0 1 0 1 0 1 1 3 3 0 0 0 2 0 0 1 1 1 3 3\n",
            " 0 2 3 2 3 0 1 3 3 3 3 2 1 0 0 0 2 3 0 1 1 1 1 3 1 0 3 0 0 3 0 2 0 3 1 1 0\n",
            " 1 0 2 3 3 0 0 3 2 0 0 3 1 0 0 0 2 3 1 1 3 1 0 3 2 1 2 0 0 1 0 0 1 1 2 3 2\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1 1 3 3 3 3 0 3 1 2 3 1 0 3 0 2 3 2 3 2 0 0 2 1 1 3\n",
            " 1 0 0 2 0 0 3 3 2 3 0 3 2 1 0 3 0 0 1 1 0 0 0 0 1 0 2 1 3 3 0 3 3 3 0 3 2\n",
            " 0 0 1 1 0 0 1 0 0 2 1 1 0 0 0 1 1 1 2 0 2 0 1 3 1 0 0 2 3 0 0 3 0 3 3 1 1\n",
            " 0 3 3 1 1 1 1 1 0 1 0 0 1 0 0 2 0 0 1 1 0 3 1 3 1 3 3 1 0 3 0 0 2 0 0 2 1\n",
            " 0 3 1 2 1 2 0 0 1 0 0 0 0 3 0 3 0 3 3 2 0 3 3 0 3 3 3 1 3 0 1 1 3 3 3 1 2\n",
            " 0 0 0 1 2 1 1 3 2 3 0 0 0 3 3 3 0 1 1 1 3 3 0 1 1 2 0 3 3 3 0 3 3 1 2 0 3\n",
            " 1 3 3 3 0 1 3 3 3 3 0 3 3 0 0 0 0 1 1 3 0 0 1 1 0 0 2 3 0 1 3 1 0 1 3 3 2\n",
            " 0 3 1 0 1 0 2 1 1 1 1 0 3 3 0 2 0 2 0 0 2 0 0 0 3 2 3 3 1 0 3 0 1 1 1 1 3\n",
            " 3 1 2 3 1 1 0 3 3 0 0 0 3 2 0 3 1 0 1 3 3 1 1 0 3 3 1 3 3 0 2 0 3 3 3 3 3\n",
            " 3 2 0 2 3 3 2 0 1 1 3 0 0 1 3 3 0 1 3 3 3 3 2 0 0 0 0 1 0 1 0 0 1 2 1 2 2\n",
            " 1 3 0 0 3 0 0 2 2 1 1 3 1 1 1 0 3 1 0 0 0 1 3 0 1 0 3 2 0 0 0 2 3 1 3 3 3\n",
            " 0 0 1 3 0 3 1 3 0 0 2 0 1 0 0 1 3 3 1 0 1 2 0 1 0 3 3 0 3 3 3 2 0 1 3 3 3\n",
            " 0 1 3 1 0 3 1 0 3 0 2 1 0 1 1 1 0 3 0 1 3 1 0 0 0 1 3 3 3 0 0 0 0 0 1 1 3\n",
            " 0 0 1 1 2 0 2 2 0 3 2 1 0 3 3 3 3 3 0 2 0 0 3 2 2 1 2 2 2 3 0 0 2 0 2 0 3\n",
            " 1 0 2 0 3 1 0 2 0 3 1 1 1 1 1 1 3 1 2 1 0 3 1 0 0 2 3 3 0 3 3 3 3 1 1 3 1\n",
            " 1 0 2 3 3 3 2 3 0 0 3 2 3 3 0 3 3 3 3 0 1 2 0 3 0 1 0 1 1 1 1 0 3 1 1 3 0\n",
            " 0 0 3 0 3 1 3 3 0 0 1 3 0 1 1 3 1 0 1 0 0 0 2 3 2 2 0 0 1 1 2 0 2 3 1 3 0\n",
            " 1 1 0 2 1 3 1 1 2 3 0 1 3 2 3 0 3 1 0 1 0 0 3 0 3 1 0 3 0 1 0 1 1 0 3 0 3\n",
            " 0 0 0 2 0 0 0 3 3 3 2 0 0 3 1 1 0 1 1 0 2 3 1 0 0 3 3 0 0 1 3 0 0 0 0 1 3\n",
            " 1 0 1 3 1 3 0 1 1 1 3 3 2 0 0 1 1 3 1 0 3 0 2 0 0 1 0 0 0 3 0 1 0 3 0 0 0\n",
            " 1 1 3 3 3 0 0 3 0 3 0 1 1 0 2 0 2 0 2 1 1 2 2 0 0 2 2 0 2 1 3 0 1 1 1 0 3\n",
            " 1 0 2 1 0 1 3 1 3 0 0 0 0 3 0 3 3 1 3 2 2 2 2 3 2 0 3 3 1 0 3 1 0 0 1 1 1\n",
            " 0 0 1 3 0 1 0 3 2 0 1 1 0 3 3 3 1 0 3 0 0 3 1 3 0 3 1 0 3 3 1 0 0 2 1 3 1\n",
            " 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes_np.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOo23YhnP8IM",
        "outputId": "93a3e689-5555-44f0-a155-722acfbe22a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to original array format\n",
        "predictions_cnn = make_original_array(predicted_classes_np)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_resnet = pd.DataFrame(predictions_cnn, columns=['ID', ' Class'])\n",
        "df_resnet.to_csv('ResNet_Attempt_reducingLR_v2.csv', index=False)"
      ],
      "metadata": {
        "id": "OsW5uY5HQDej"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_resnet_0=pd.read_csv('ResNet_Attempt_reducingLR.csv')"
      ],
      "metadata": {
        "id": "kA_pe0d_QVJH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_resnet_1=pd.read_csv('ResNet_Attempt_2_ep7.csv')"
      ],
      "metadata": {
        "id": "knyzyvg-dwnk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the predicted classes are in a column named 'Class'\n",
        "differences = (df_resnet_0[' Class'] != df_resnet[' Class']).sum()\n",
        "total = len(df_resnet)\n",
        "\n",
        "print(f\"Total Predictions: {total}\")\n",
        "print(f\"Different Predictions: {differences}\")\n",
        "print(f\"Percentage Difference: {(differences / total) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ekZRLdIQZev",
        "outputId": "a67ddf64-03dd-43ac-bbd4-10443d8551dc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Predictions: 1000\n",
            "Different Predictions: 122\n",
            "Percentage Difference: 12.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of each class\n",
        "class_counts = df_resnet[' Class'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Counts for each class in latest ResNet:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Optional: Convert to dictionary for further use\n",
        "class_counts_dict = class_counts.to_dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5tZLx4nEr2K",
        "outputId": "6dbf312c-9b31-4e1e-a909-b0ff2becc52a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for each class in latest ResNet:\n",
            " Class\n",
            "0    360\n",
            "3    296\n",
            "1    232\n",
            "2    112\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of each class\n",
        "class_counts = df_resnet_0[' Class'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Counts for each class in first ResNet:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Optional: Convert to dictionary for further use\n",
        "class_counts_dict = class_counts.to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnMoHJrfFXGA",
        "outputId": "50d1a6ec-29f9-4071-9ed3-4bbfaeaec927"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for each class in first ResNet:\n",
            " Class\n",
            "0    342\n",
            "3    299\n",
            "1    261\n",
            "2     98\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of each class\n",
        "class_counts = df_resnet_1[' Class'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Counts for each class in 2nd ResNet:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Optional: Convert to dictionary for further use\n",
        "class_counts_dict = class_counts.to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8ys1G7PFfoN",
        "outputId": "78991d94-52ba-46b8-d8d1-7803c80ecf5e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for each class in 2nd ResNet:\n",
            " Class\n",
            "0    359\n",
            "1    268\n",
            "3    254\n",
            "2    119\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( np.array(df_cnn_augb[1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB0RPkfMQkvI",
        "outputId": "64a5ce8f-c995-413c-c98a-2cdff9d0310c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   2    0]\n",
            " [   3    3]\n",
            " [   4    3]\n",
            " ...\n",
            " [ 998    3]\n",
            " [ 999    3]\n",
            " [1000    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_resnet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp9SnHcXQxC7",
        "outputId": "d8b0ae62-4d0b-4eb0-fe6b-b210cc7dd863"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ID   Class\n",
            "0       1       3\n",
            "1       2       0\n",
            "2       3       3\n",
            "3       4       3\n",
            "4       5       0\n",
            "..    ...     ...\n",
            "995   996       2\n",
            "996   997       1\n",
            "997   998       3\n",
            "998   999       1\n",
            "999  1000       0\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR= 1.5e-4 and epochs = 12"
      ],
      "metadata": {
        "id": "U2OVTGZ3thW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "np.random.seed(42)\n",
        "num_samples = images.shape[0]\n",
        "\n",
        "indices = np.arange(num_samples)\n",
        "np.random.shuffle(indices)\n",
        "shuffled_images = images[indices]\n",
        "shuffled_labels = labels[indices]\n",
        "\n",
        "# Split into training and validation sets\n",
        "baseline1Idxs = int(num_samples * 0.8)  # 80% for training, 20% for validation\n",
        "split_index = int(baseline1Idxs * 0.8)\n",
        "\n",
        "train_images = shuffled_images[:split_index]\n",
        "train_labels = shuffled_labels[:split_index]\n",
        "\n",
        "val_images = shuffled_images[split_index:]\n",
        "val_labels = shuffled_labels[split_index:]\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"Train Images: {train_images.shape}, Train Labels: {train_labels.shape}\")\n",
        "print(f\"Validation Images: {val_images.shape}, Validation Labels: {val_labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aj7vYp1tgdo",
        "outputId": "a2474cd8-d669-4482-defa-472092e9372e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Images: (62384, 28, 28), Train Labels: (62384,)\n",
            "Validation Images: (35093, 28, 28), Validation Labels: (35093,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine whether to use GPU or CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "JuD2syxEuETX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Data Augmentation\n",
        "augment_gen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Augment images with labels 1 and 2\n",
        "mask1 = np.isin(train_labels, [1, 2])\n",
        "filtered_images_1_2 = train_images[mask1]\n",
        "filtered_labels_1_2 = train_labels[mask1]\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for img, lbl in zip(filtered_images_1_2, filtered_labels_1_2):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
        "    for _ in range(10):  # Generate 10 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Augment images with labels 0 and 3\n",
        "mask2 = np.isin(train_labels, [0, 3])\n",
        "filtered_images_0_3 = train_images[mask2]\n",
        "filtered_labels_0_3 = train_labels[mask2]\n",
        "\n",
        "for img, lbl in zip(filtered_images_0_3, filtered_labels_0_3):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
        "    for _ in range(2):  # Generate 2 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Combine augmented data with original training data\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "train_images_cnn = np.expand_dims(train_images, axis=-1)  # Add channel dimension to training images\n",
        "train_images_cnn = np.concatenate((train_images_cnn, augmented_images), axis=0)\n",
        "train_labels_cnn = np.concatenate((train_labels, augmented_labels), axis=0)\n",
        "\n",
        "# Convert labels to class indices\n",
        "train_labels_cnn = train_labels_cnn.astype(int)\n",
        "val_labels = val_labels.astype(int)\n",
        "\n",
        "# PyTorch Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_images_cnn, train_labels_cnn, transform=transform)\n",
        "val_dataset = CustomDataset(val_images, val_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Bottleneck Block for ResNet-50\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * 4, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(x)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet-50 for 28x28 Inputs\n",
        "class ResNet28x28(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=4):\n",
        "        super(ResNet28x28, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Adjusted for 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1)  # No stride for 28x28 input\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * 4, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * 4:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * 4),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * 4\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Compute Class Weights for Balanced Loss\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_cnn), y=train_labels_cnn)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "# Temperature Scaled Cross Entropy Loss\n",
        "class TemperatureScaledCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, temperature=2.0):\n",
        "        super(TemperatureScaledCrossEntropyLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        scaled_logits = logits / self.temperature\n",
        "        return self.criterion(scaled_logits, targets)\n",
        "\n",
        "\n",
        "# Training Loop with AUC Calculation\n",
        "def train_resnet28(model, train_loader, val_loader, epochs=10):\n",
        "    criterion = TemperatureScaledCrossEntropyLoss(temperature=2.0)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1.5e-4, weight_decay=1e-5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "\n",
        "                # Store predicted probabilities and true class indices\n",
        "                all_preds.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate AUC\n",
        "        val_auc = roc_auc_score(\n",
        "            np.eye(4)[np.array(all_labels)],  # Convert class indices to one-hot encoded for AUC\n",
        "            np.array(all_preds),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "\n",
        "        # Calculate Metrics\n",
        "        train_accuracy = train_correct / len(train_loader.dataset)\n",
        "        val_accuracy = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "\n",
        "# Instantiate Model and Train\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet28x28(Bottleneck, [3, 4, 6, 3], num_classes=4).to(device)\n",
        "\n",
        "train_resnet28(model, train_loader, val_loader, epochs=12)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "DJgEGIpkROPa",
        "outputId": "d5b51da1-b753-4860-e482-7ae342403fd7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [64, 64, 3, 3], expected input[32, 256, 28, 28] to have 64 channels, but got 256 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-5598dc170a42>\u001b[0m in \u001b[0;36m<cell line: 250>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet28x28\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m \u001b[0mtrain_resnet28\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-5598dc170a42>\u001b[0m in \u001b[0;36mtrain_resnet28\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5598dc170a42>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5598dc170a42>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 64, 3, 3], expected input[32, 256, 28, 28] to have 64 channels, but got 256 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG Net Kinda implementation"
      ],
      "metadata": {
        "id": "0LM74DR5AWH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Data Augmentation\n",
        "augment_gen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Augment images with labels 1 and 2\n",
        "mask1 = np.isin(train_labels, [1, 2])\n",
        "filtered_images_1_2 = train_images[mask1]\n",
        "filtered_labels_1_2 = train_labels[mask1]\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for img, lbl in zip(filtered_images_1_2, filtered_labels_1_2):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
        "    for _ in range(10):  # Generate 10 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Augment images with labels 0 and 3\n",
        "mask2 = np.isin(train_labels, [0, 3])\n",
        "filtered_images_0_3 = train_images[mask2]\n",
        "filtered_labels_0_3 = train_labels[mask2]\n",
        "\n",
        "for img, lbl in zip(filtered_images_0_3, filtered_labels_0_3):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
        "    for _ in range(2):  # Generate 2 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Combine augmented data with original training data\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "train_images_cnn = np.expand_dims(train_images, axis=-1)  # Add channel dimension to training images\n",
        "train_images_cnn = np.concatenate((train_images_cnn, augmented_images), axis=0)\n",
        "train_labels_cnn = np.concatenate((train_labels, augmented_labels), axis=0)\n",
        "\n",
        "# Convert labels to class indices\n",
        "train_labels_cnn = train_labels_cnn.astype(int)\n",
        "val_labels = val_labels.astype(int)\n",
        "\n",
        "# PyTorch Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_images_cnn, train_labels_cnn, transform=transform)\n",
        "val_dataset = CustomDataset(val_images, val_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# VGG-like Model\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Convolutional Layer 1\n",
        "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 64x14x14\n",
        "\n",
        "            # Convolutional Layer 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Output: 128x7x7\n",
        "\n",
        "            # Convolutional Layer 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # Output: 256x3x3\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),  # Flatten the output of convolutional layers\n",
        "            nn.Linear(256 * 3 * 3, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Compute Class Weights for Balanced Loss\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_cnn), y=train_labels_cnn)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Training Loop with AUC Calculation\n",
        "def train_vgg(model, train_loader, val_loader, device, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=8e-5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                all_preds.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate AUC\n",
        "        val_auc = roc_auc_score(\n",
        "            np.eye(4)[np.array(all_labels)],  # One-hot encode true labels\n",
        "            np.array(all_preds),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "\n",
        "        # Metrics\n",
        "        train_accuracy = train_correct / len(train_loader.dataset)\n",
        "        val_accuracy = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "\n",
        "# Instantiate the model and train\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ImprovedCNN(num_classes=4).to(device)\n",
        "\n",
        "train_vgg(model, train_loader, val_loader, device, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLz7hKewtQ8F",
        "outputId": "49e33c86-7ffa-4586-83be-49266f24ca8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 6530.0767, Train Accuracy: 0.6981, Val Loss: 517.0303, Val Accuracy: 0.8465, Val AUC: 0.9360\n",
            "Epoch 2/20, Train Loss: 5018.5268, Train Accuracy: 0.7751, Val Loss: 414.7830, Val Accuracy: 0.8679, Val AUC: 0.9583\n",
            "Epoch 3/20, Train Loss: 4475.6343, Train Accuracy: 0.7994, Val Loss: 394.4203, Val Accuracy: 0.8740, Val AUC: 0.9589\n",
            "Epoch 4/20, Train Loss: 4054.8730, Train Accuracy: 0.8180, Val Loss: 374.5135, Val Accuracy: 0.8891, Val AUC: 0.9634\n",
            "Epoch 5/20, Train Loss: 3693.6781, Train Accuracy: 0.8347, Val Loss: 402.5338, Val Accuracy: 0.8808, Val AUC: 0.9632\n",
            "Epoch 6/20, Train Loss: 3339.1440, Train Accuracy: 0.8505, Val Loss: 353.4662, Val Accuracy: 0.8950, Val AUC: 0.9656\n",
            "Epoch 7/20, Train Loss: 2996.5623, Train Accuracy: 0.8653, Val Loss: 369.1341, Val Accuracy: 0.8928, Val AUC: 0.9670\n",
            "Epoch 8/20, Train Loss: 2660.1295, Train Accuracy: 0.8796, Val Loss: 384.7228, Val Accuracy: 0.8886, Val AUC: 0.9664\n",
            "Epoch 9/20, Train Loss: 2349.5072, Train Accuracy: 0.8930, Val Loss: 421.3677, Val Accuracy: 0.8954, Val AUC: 0.9671\n",
            "Epoch 10/20, Train Loss: 2059.6331, Train Accuracy: 0.9065, Val Loss: 447.8722, Val Accuracy: 0.8959, Val AUC: 0.9670\n",
            "Epoch 11/20, Train Loss: 1789.2282, Train Accuracy: 0.9182, Val Loss: 476.7006, Val Accuracy: 0.8935, Val AUC: 0.9659\n",
            "Epoch 12/20, Train Loss: 1560.3600, Train Accuracy: 0.9289, Val Loss: 468.2778, Val Accuracy: 0.8930, Val AUC: 0.9666\n",
            "Epoch 13/20, Train Loss: 1362.5406, Train Accuracy: 0.9382, Val Loss: 523.0434, Val Accuracy: 0.8977, Val AUC: 0.9645\n",
            "Epoch 14/20, Train Loss: 1193.9432, Train Accuracy: 0.9458, Val Loss: 524.5140, Val Accuracy: 0.8928, Val AUC: 0.9650\n",
            "Epoch 15/20, Train Loss: 1057.2295, Train Accuracy: 0.9522, Val Loss: 607.6482, Val Accuracy: 0.8925, Val AUC: 0.9651\n",
            "Epoch 16/20, Train Loss: 945.4943, Train Accuracy: 0.9577, Val Loss: 580.3532, Val Accuracy: 0.8907, Val AUC: 0.9647\n",
            "Epoch 17/20, Train Loss: 856.1123, Train Accuracy: 0.9625, Val Loss: 660.2989, Val Accuracy: 0.8943, Val AUC: 0.9634\n",
            "Epoch 18/20, Train Loss: 778.2376, Train Accuracy: 0.9660, Val Loss: 626.8299, Val Accuracy: 0.8873, Val AUC: 0.9651\n",
            "Epoch 19/20, Train Loss: 708.7436, Train Accuracy: 0.9692, Val Loss: 663.3658, Val Accuracy: 0.8870, Val AUC: 0.9637\n",
            "Epoch 20/20, Train Loss: 662.4692, Train Accuracy: 0.9714, Val Loss: 694.4057, Val Accuracy: 0.8859, Val AUC: 0.9644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "# Assuming testimages is a NumPy array of shape (num_samples, 28, 28)\n",
        "\n",
        "# Preprocess test images\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Ensure same normalization as during training\n",
        "])\n",
        "\n",
        "# Convert testimages to PyTorch Tensor with proper dimensions\n",
        "test_images_tensor = torch.stack([test_transform(img) for img in testimages])  # Shape: (num_samples, 1, 28, 28)\n",
        "\n",
        "# Send to device (e.g., GPU or CPU)\n",
        "test_images_tensor = test_images_tensor.to(device)\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    logits = model(test_images_tensor)  # Shape: (num_samples, num_classes)\n",
        "    probabilities = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
        "    predicted_classes = torch.argmax(probabilities, dim=1)  # Get class with highest probability\n",
        "\n",
        "# Convert predictions to NumPy array\n",
        "predicted_classes_np = predicted_classes.cpu().numpy()\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predicted Classes:\", predicted_classes_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SHl0TFbCTzk",
        "outputId": "91524e0d-4429-44d8-8147-3235253dd8c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Classes: [3 0 3 3 0 2 1 0 0 3 0 1 3 3 3 2 0 1 0 0 1 0 2 3 0 0 0 1 3 1 3 1 3 3 2 1 0\n",
            " 3 1 0 1 0 0 0 0 0 1 1 1 3 1 0 1 0 0 0 0 1 0 1 1 3 3 2 0 0 0 0 0 1 0 1 1 3\n",
            " 0 2 3 2 3 0 1 3 3 1 3 2 1 2 2 0 0 3 0 1 1 0 0 3 1 0 3 0 0 3 0 2 2 3 1 2 0\n",
            " 1 0 2 3 3 0 0 0 2 0 0 3 1 0 0 0 0 3 1 1 3 1 0 1 2 1 2 0 0 1 0 0 1 1 2 3 2\n",
            " 0 1 1 0 0 1 1 0 0 0 0 1 3 3 3 3 3 0 3 1 3 3 1 0 3 0 2 3 2 3 0 0 2 2 1 1 2\n",
            " 2 0 0 2 0 0 3 3 2 3 0 3 2 1 2 3 0 0 1 1 0 1 0 0 1 3 2 1 1 3 0 3 1 3 0 3 3\n",
            " 0 0 2 3 0 0 3 0 0 0 1 1 0 0 0 1 1 1 2 2 2 0 1 3 1 0 0 1 3 0 0 3 0 3 3 1 1\n",
            " 0 1 3 1 1 0 1 1 1 0 0 1 1 0 0 2 0 0 1 1 0 3 1 3 1 3 2 1 0 3 0 0 2 0 0 2 1\n",
            " 0 3 1 1 0 3 0 0 1 0 0 0 0 3 0 3 0 3 3 1 0 0 3 0 3 3 0 3 3 0 1 1 1 3 3 3 2\n",
            " 0 0 2 1 2 0 1 3 3 1 0 0 0 3 3 3 2 1 1 0 3 3 0 1 1 2 0 3 3 3 0 3 3 0 2 1 3\n",
            " 1 3 3 1 1 1 3 3 3 3 1 3 3 0 0 2 0 1 1 3 0 0 1 1 0 0 2 3 0 1 3 1 0 0 3 3 2\n",
            " 0 3 1 0 1 0 2 1 3 1 1 0 3 3 0 1 0 2 0 0 0 0 0 0 3 2 3 3 1 3 3 0 1 1 1 1 3\n",
            " 3 1 2 3 1 0 2 3 3 0 0 0 2 2 0 3 0 0 1 3 1 1 1 0 3 3 1 3 3 3 2 1 1 1 3 3 3\n",
            " 3 2 0 3 3 3 3 0 1 1 3 0 0 1 1 3 0 0 1 3 3 3 3 0 0 0 0 0 0 1 0 0 0 2 1 2 0\n",
            " 1 3 0 0 1 0 0 3 2 1 1 3 1 1 1 0 3 1 0 2 0 1 2 0 1 0 3 3 0 0 0 2 3 1 1 3 3\n",
            " 0 0 0 3 0 2 1 3 0 0 2 0 1 3 0 1 3 3 1 0 1 2 0 1 0 3 3 0 2 1 3 2 0 0 3 3 3\n",
            " 0 1 3 1 0 3 3 1 3 0 2 1 0 1 3 1 0 0 0 1 3 2 2 0 0 1 3 0 3 0 0 0 0 0 1 1 3\n",
            " 0 0 1 1 1 0 2 1 0 3 2 1 0 3 3 3 3 3 0 3 0 0 2 3 3 1 2 2 0 1 0 0 0 0 2 0 3\n",
            " 1 0 2 1 2 1 2 2 0 3 1 1 1 1 1 1 2 1 0 1 0 3 1 0 3 2 3 2 0 3 3 3 3 1 1 3 1\n",
            " 1 0 2 3 3 3 2 3 0 2 0 0 3 1 0 1 3 3 0 0 1 1 0 3 1 1 0 1 1 0 1 3 1 1 1 0 2\n",
            " 0 0 3 0 3 1 3 3 0 0 1 3 0 1 1 3 0 0 1 0 0 0 3 1 2 0 0 0 0 1 3 0 2 3 1 3 0\n",
            " 1 1 0 1 1 3 0 1 2 3 0 1 1 2 3 0 3 1 0 1 0 0 0 0 3 1 0 3 0 1 0 1 1 0 1 3 1\n",
            " 0 0 0 1 0 0 0 3 3 3 2 0 1 3 1 0 1 1 0 0 2 3 1 0 0 1 0 0 0 1 3 0 0 0 0 0 3\n",
            " 1 0 1 2 2 1 0 1 1 1 3 3 1 0 0 1 1 3 1 0 3 0 0 0 0 1 0 2 0 1 0 1 0 3 0 0 2\n",
            " 0 1 3 3 3 0 2 3 0 1 0 1 1 0 0 2 2 0 2 1 3 0 2 0 0 3 2 0 2 3 3 0 1 1 1 0 3\n",
            " 1 0 2 1 2 1 3 1 3 0 0 3 0 3 0 3 3 1 3 2 2 3 2 3 2 1 1 3 1 0 3 1 0 0 1 1 1\n",
            " 0 0 1 3 0 1 0 3 0 0 1 1 0 1 3 3 1 0 1 0 0 3 1 3 1 3 1 0 3 3 1 0 0 3 1 3 1\n",
            " 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_resnet_1=pd.read_csv('ResNet_Attempt_2_ep7.csv')\n",
        "df_resnet=pd.read_csv('ResNet_Attempt.csv')\n",
        "df_resnet_2=pd.read_csv('ResNet_Attempt_1.5_ep12.csv')\n",
        "# Convert predictions to original array format\n",
        "predictions_cnn = make_original_array(predicted_classes_np)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df_vgg = pd.DataFrame(predictions_cnn, columns=['ID', ' Class'])\n",
        "df_vgg.to_csv('VggNet.csv', index=False)\n"
      ],
      "metadata": {
        "id": "XVVVRCzCOIeW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWejAS04O2VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the predicted classes are in a column named 'Class'\n",
        "differences = (df_vgg[' Class'] != df_resnet[' Class']).sum()\n",
        "total = len(df_resnet)\n",
        "\n",
        "print(f\"Total Predictions: {total}\")\n",
        "print(f\"Different Predictions: {differences}\")\n",
        "print(f\"Percentage Difference: {(differences / total) * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad319cf-05d8-40d7-9ced-87de66246546",
        "id": "CGUuGfktPPCN"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Predictions: 1000\n",
            "Different Predictions: 196\n",
            "Percentage Difference: 19.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of each class\n",
        "class_counts = df_resnet[' Class'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Counts for each class in latest ResNet:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Optional: Convert to dictionary for further use\n",
        "class_counts_dict = class_counts.to_dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36dda0b6-3224-4331-c7d7-65091a906b5c",
        "id": "pXDJgTXgPPCO"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for each class in latest ResNet:\n",
            " Class\n",
            "0    342\n",
            "3    299\n",
            "1    261\n",
            "2     98\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of each class\n",
        "class_counts = df_resnet_1[' Class'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Counts for each class in first ResNet:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Optional: Convert to dictionary for further use\n",
        "class_counts_dict = class_counts.to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29b1d772-22c8-4d39-b26c-6283d584471a",
        "id": "sKicV_CTPPCO"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for each class in first ResNet:\n",
            " Class\n",
            "0    359\n",
            "1    268\n",
            "3    254\n",
            "2    119\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of each class\n",
        "class_counts = df_resnet_2[' Class'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Counts for each class in 2nd ResNet:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Optional: Convert to dictionary for further use\n",
        "class_counts_dict = class_counts.to_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176ee7f2-dcaf-4f16-9d93-e134fcdf0ddd",
        "id": "1xRR0vkJPPCO"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for each class in 2nd ResNet:\n",
            " Class\n",
            "0    318\n",
            "3    298\n",
            "1    277\n",
            "2    107\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the count of each class\n",
        "class_counts = df_vgg[' Class'].value_counts()\n",
        "\n",
        "# Display the counts\n",
        "print(\"Counts for each class in latest ResNet:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Optional: Convert to dictionary for further use\n",
        "class_counts_dict = class_counts.to_dict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEjzaJLRPX2F",
        "outputId": "dbd2d86b-74ed-4e3f-f8f8-78f912071780"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for each class in latest ResNet:\n",
            " Class\n",
            "0    357\n",
            "1    274\n",
            "3    259\n",
            "2    110\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ROFfOrgPeVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision transformer updated"
      ],
      "metadata": {
        "id": "odWkRPpsRqE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import timm\n",
        "\n",
        "\n",
        "# Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Data Augmentation\n",
        "augment_gen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# Augment images with labels 1 and 2\n",
        "mask1 = np.isin(train_labels, [1, 2])\n",
        "filtered_images_1_2 = train_images[mask1]\n",
        "filtered_labels_1_2 = train_labels[mask1]\n",
        "\n",
        "augmented_images = []\n",
        "augmented_labels = []\n",
        "\n",
        "for img, lbl in zip(filtered_images_1_2, filtered_labels_1_2):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
        "    for _ in range(10):  # Generate 10 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Augment images with labels 0 and 3\n",
        "mask2 = np.isin(train_labels, [0, 3])\n",
        "filtered_images_0_3 = train_images[mask2]\n",
        "filtered_labels_0_3 = train_labels[mask2]\n",
        "\n",
        "for img, lbl in zip(filtered_images_0_3, filtered_labels_0_3):\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)   # Add batch dimension\n",
        "    for _ in range(2):  # Generate 2 augmented samples per image\n",
        "        augmented = next(augment_gen.flow(img, batch_size=1))[0]\n",
        "        augmented_images.append(augmented)\n",
        "        augmented_labels.append(lbl)\n",
        "\n",
        "# Combine augmented data with original training data\n",
        "augmented_images = np.array(augmented_images)\n",
        "augmented_labels = np.array(augmented_labels)\n",
        "train_images_vit = np.expand_dims(train_images, axis=-1)  # Add channel dimension to training images\n",
        "train_images_vit = np.concatenate((train_images_vit, augmented_images), axis=0)\n",
        "train_labels_vit = np.concatenate((train_labels, augmented_labels), axis=0)\n",
        "\n",
        "# Convert labels to class indices\n",
        "train_labels_vit = train_labels_vit.astype(int)\n",
        "val_labels = val_labels.astype(int)\n",
        "\n",
        "# PyTorch Dataset and DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to 3 channels for ViT\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_images_vit, train_labels_vit, transform=transform)\n",
        "val_dataset = CustomDataset(val_images, val_labels, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Vision Transformer Model for 28x28 Images\n",
        "class ViTModel28x28(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(ViTModel28x28, self).__init__()\n",
        "        self.vit = timm.create_model(\n",
        "            'vit_base_patch16_224',\n",
        "            pretrained=True,\n",
        "            img_size=28,\n",
        "            patch_size=7\n",
        "        )\n",
        "        self.vit.head = nn.Linear(self.vit.head.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)\n",
        "\n",
        "\n",
        "# Compute Class Weights for Balanced Loss\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels_vit), y=train_labels_vit)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "\n",
        "# Training Loop with AUC Calculation\n",
        "def train_vit(model, train_loader, val_loader, epochs=10):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_correct += (preds == labels).sum().item()\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                all_preds.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate AUC\n",
        "        val_auc = roc_auc_score(\n",
        "            np.eye(4)[np.array(all_labels)],  # Convert class indices to one-hot encoded for AUC\n",
        "            np.array(all_preds),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "\n",
        "        # Metrics\n",
        "        train_accuracy = train_correct / len(train_loader.dataset)\n",
        "        val_accuracy = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "\n",
        "# Instantiate and Train Model\n",
        "model = ViTModel28x28(num_classes=4).to(device)\n",
        "train_vit(model, train_loader, val_loader, epochs=12)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361,
          "referenced_widgets": [
            "f255e113b6d04e70b52a92b4ef0cb0cd",
            "d96485893f4540fd82e6dc421a4c3d86",
            "7caf90e99cbf4e469fa721eed3649139",
            "bfa95413c3794ef98f8639404aeaea8b",
            "f33e45111c1e49d9840bf26039b7b84d",
            "32b9d7e222854de7b1dc1b69ea6464ce",
            "2237ce6aa4cd43698de7a0b9b8e84c46",
            "06442c0f3865459b9e11485c06835410",
            "5b1428264c634db3a55f27587cba3a6d",
            "1529029ab82943a79fc391c6ba074093",
            "5158a120fabb48c981f4a256f9f6697c"
          ]
        },
        "id": "wgfhjsN2Rrxf",
        "outputId": "6059a486-4f23-4b05-b033-ac0860bac750"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f255e113b6d04e70b52a92b4ef0cb0cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12, Train Loss: 6572.6255, Train Accuracy: 0.6929, Val Loss: 514.6851, Val Accuracy: 0.8535, Val AUC: 0.9343\n",
            "Epoch 2/12, Train Loss: 5195.8592, Train Accuracy: 0.7628, Val Loss: 465.3332, Val Accuracy: 0.8498, Val AUC: 0.9502\n",
            "Epoch 3/12, Train Loss: 4563.4953, Train Accuracy: 0.7930, Val Loss: 436.7499, Val Accuracy: 0.8678, Val AUC: 0.9526\n",
            "Epoch 4/12, Train Loss: 4058.6413, Train Accuracy: 0.8154, Val Loss: 435.0876, Val Accuracy: 0.8705, Val AUC: 0.9506\n",
            "Epoch 5/12, Train Loss: 3586.2547, Train Accuracy: 0.8368, Val Loss: 425.9187, Val Accuracy: 0.8711, Val AUC: 0.9563\n",
            "Epoch 6/12, Train Loss: 3135.0203, Train Accuracy: 0.8575, Val Loss: 440.3009, Val Accuracy: 0.8624, Val AUC: 0.9542\n",
            "Epoch 7/12, Train Loss: 2721.2417, Train Accuracy: 0.8757, Val Loss: 449.7716, Val Accuracy: 0.8686, Val AUC: 0.9562\n",
            "Epoch 8/12, Train Loss: 2345.7958, Train Accuracy: 0.8937, Val Loss: 495.6981, Val Accuracy: 0.8713, Val AUC: 0.9537\n",
            "Epoch 9/12, Train Loss: 2027.4306, Train Accuracy: 0.9078, Val Loss: 514.2214, Val Accuracy: 0.8717, Val AUC: 0.9536\n",
            "Epoch 10/12, Train Loss: 1746.2774, Train Accuracy: 0.9212, Val Loss: 547.8999, Val Accuracy: 0.8718, Val AUC: 0.9536\n",
            "Epoch 11/12, Train Loss: 1520.3833, Train Accuracy: 0.9320, Val Loss: 546.4801, Val Accuracy: 0.8687, Val AUC: 0.9539\n",
            "Epoch 12/12, Train Loss: 1341.2824, Train Accuracy: 0.9404, Val Loss: 596.9084, Val Accuracy: 0.8750, Val AUC: 0.9524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2whJiaVMRsTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f255e113b6d04e70b52a92b4ef0cb0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d96485893f4540fd82e6dc421a4c3d86",
              "IPY_MODEL_7caf90e99cbf4e469fa721eed3649139",
              "IPY_MODEL_bfa95413c3794ef98f8639404aeaea8b"
            ],
            "layout": "IPY_MODEL_f33e45111c1e49d9840bf26039b7b84d"
          }
        },
        "d96485893f4540fd82e6dc421a4c3d86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b9d7e222854de7b1dc1b69ea6464ce",
            "placeholder": "​",
            "style": "IPY_MODEL_2237ce6aa4cd43698de7a0b9b8e84c46",
            "value": "model.safetensors: 100%"
          }
        },
        "7caf90e99cbf4e469fa721eed3649139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06442c0f3865459b9e11485c06835410",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b1428264c634db3a55f27587cba3a6d",
            "value": 346284714
          }
        },
        "bfa95413c3794ef98f8639404aeaea8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1529029ab82943a79fc391c6ba074093",
            "placeholder": "​",
            "style": "IPY_MODEL_5158a120fabb48c981f4a256f9f6697c",
            "value": " 346M/346M [00:01&lt;00:00, 243MB/s]"
          }
        },
        "f33e45111c1e49d9840bf26039b7b84d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b9d7e222854de7b1dc1b69ea6464ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2237ce6aa4cd43698de7a0b9b8e84c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06442c0f3865459b9e11485c06835410": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b1428264c634db3a55f27587cba3a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1529029ab82943a79fc391c6ba074093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5158a120fabb48c981f4a256f9f6697c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}